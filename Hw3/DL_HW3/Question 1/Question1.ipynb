{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ8JspU3EtWF"
      },
      "source": [
        "1. Mount Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7N-nkviBvHG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1281a0c-c889-45c9-fbe7-2aaa0bdf4c58"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao-Z2oU_PRPR"
      },
      "source": [
        "2. Change Runtime to \"GPU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmvnOi18E0kT"
      },
      "source": [
        "3. Test a sample using the pre-trained ResNet101"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfHeefFyM-Sr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af05413f-0e82-456a-9939-33071072e85b"
      },
      "source": [
        "import torch, torchvision\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "# load model\r\n",
        "resnet = torchvision.models.resnet101(pretrained=True)\r\n",
        "\r\n",
        "# set network to evaluation mode\r\n",
        "resnet.eval()\r\n",
        "\r\n",
        "transform = torchvision.transforms.Compose([          \r\n",
        " torchvision.transforms.Resize(256),                   \r\n",
        " torchvision.transforms.CenterCrop(224),               \r\n",
        " torchvision.transforms.ToTensor(),                     \r\n",
        " torchvision.transforms.Normalize(                      \r\n",
        " mean=[0.485, 0.456, 0.406],                            \r\n",
        " std=[0.229, 0.224, 0.225]                             \r\n",
        " )])\r\n",
        "\r\n",
        "\r\n",
        "img = Image.open(\"/content/gdrive/MyDrive/SUTD_GSUITE/Year 3/Term 7/DeepLearning/Hw3/dog.jpg\") # You can download an image of a dog from Internet or capture an image by yourself.\r\n",
        "img_t = transform(img)\r\n",
        "print(img_t.shape)\r\n",
        "\r\n",
        "batch_t = torch.unsqueeze(img_t, 0)\r\n",
        "\r\n",
        "\r\n",
        "# perform inference\r\n",
        "out = resnet(batch_t)\r\n",
        "\r\n",
        "# print top-5 classes predicted by model\r\n",
        "_, indices = torch.sort(out, descending=True)\r\n",
        "percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\r\n",
        "for idx in indices[0][:5]:\r\n",
        "  print('Label:', idx, '. Confidence Score:', percentage[idx].item(), '%')\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 224, 224])\n",
            "Label: tensor(259) . Confidence Score: 64.96243286132812 %\n",
            "Label: tensor(153) . Confidence Score: 18.492586135864258 %\n",
            "Label: tensor(258) . Confidence Score: 3.901040554046631 %\n",
            "Label: tensor(152) . Confidence Score: 3.283771514892578 %\n",
            "Label: tensor(265) . Confidence Score: 1.451918601989746 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pge3-ER1M-Ss"
      },
      "source": [
        "4. Refer to https://gist.github.com/ageitgey/4e1342c10a71981d0b491e1b8227328b, to check if the predicted classes are meaningful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-kkOJ6hNbOB"
      },
      "source": [
        "5. Task\r\n",
        "\r\n",
        "Modify the code above, to peform data augmentation for the testing sample (averaging the scores of 5 crops: center crop, upper left crop, lower left crop, lower right crop, upper right crop).\r\n",
        "\r\n",
        "Pls briefly discuss the advantages and disadvantages of using testing data augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q95E2uxj4-T1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "538ebfb4-90b5-4f81-c58e-03cb8eccdf15"
      },
      "source": [
        "import torch, torchvision\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "def perRestNet(img_t):\n",
        "    batch_t = torch.unsqueeze(img_t, 0)\n",
        "\n",
        "    # perform inference\n",
        "    out = resnet(batch_t)\n",
        "    return out\n",
        "\n",
        "\n",
        "# load model\n",
        "resnet = torchvision.models.resnet101(pretrained=True)\n",
        "\n",
        "# set network to evaluation mode\n",
        "resnet.eval()\n",
        "\n",
        "img = Image.open(\"/content/gdrive/MyDrive/SUTD_GSUITE/Year 3/Term 7/DeepLearning/Hw3/dog.jpg\") # You can download an image of a dog from Internet or capture an image by yourself.\n",
        "\n",
        "\n",
        "########### Not allowed to use inbuild function, but it works for references ############\n",
        "\n",
        "# transform2 = torchvision.transforms.Compose([transforms.Resize(256),\n",
        "#                             transforms.FiveCrop(224), # this is a list of PIL Images\n",
        "#                             transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
        "#                             transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "#                            ])\n",
        "\n",
        "# img_t = transform2(img)\n",
        "# print(img_t.shape)\n",
        "\n",
        "# # perform inference\n",
        "# out = resnet(img_t)\n",
        "\n",
        "# # print top-5 classes predicted by model\n",
        "# _, indices = torch.sort(out, descending=True)\n",
        "# percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
        "# for idx in indices[0][:5]:\n",
        "#   print('Label:', idx, '. Confidence Score:', percentage[idx].item(), '%')\n",
        "\n",
        "transform3 = torchvision.transforms.Compose([       \n",
        " torchvision.transforms.Resize(256),                   \n",
        " torchvision.transforms.ToTensor(),                     \n",
        " torchvision.transforms.Normalize(                      \n",
        " mean=[0.485, 0.456, 0.406],                            \n",
        " std=[0.229, 0.224, 0.225]                             \n",
        " )])\n",
        "\n",
        "img_t = transform3(img)\n",
        "scores_sum = 0\n",
        "outputTorch = torch.zeros((1,1000))\n",
        "\n",
        "for i in range(0,5):\n",
        "    slicedIMG = img_t\n",
        "    if(i==0):\n",
        "        slicedIMG = img_t[:,0:224,0:224] ## torch.Size([3, 224, 224])\n",
        "    if(i==1):\n",
        "        slicedIMG = img_t[:,0:224,img_t.shape[2]-224:img_t.shape[2]]  ## torch.Size([3, 224, 224])\n",
        "    if(i==2):\n",
        "        slicedIMG = img_t[:,img_t.shape[1]-224:img_t.shape[1],0:224]\n",
        "    if(i==3):\n",
        "        slicedIMG = img_t[:,img_t.shape[1]-224:img_t.shape[1],img_t.shape[2]-224:img_t.shape[2]]\n",
        "    if(i==4):\n",
        "        slicedIMG = img_t[:,round(img_t.shape[1]/2-112):round(img_t.shape[1]/2+112),round(img_t.shape[2]/2-112):round(img_t.shape[2]/2+112)]\n",
        "        \n",
        "    outputTorch = outputTorch.add(perRestNet(slicedIMG))\n",
        "\n",
        "outputTorch = torch.div(outputTorch,5)\n",
        "\n",
        "print(\"Averaging scores of 5 crops:\")\n",
        "# print top-5 classes predicted by model\n",
        "_, indices = torch.sort(outputTorch, descending=True)\n",
        "percentage = torch.nn.functional.softmax(outputTorch, dim=1)[0] * 100\n",
        "for idx in indices[0][:5]:\n",
        "    print('Label:', idx, '. Confidence Score:', percentage[idx].item(), '%')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Averaging scores of 5 crops:\n",
            "Label: tensor(259) . Confidence Score: 67.27539825439453 %\n",
            "Label: tensor(153) . Confidence Score: 16.193790435791016 %\n",
            "Label: tensor(258) . Confidence Score: 5.253355026245117 %\n",
            "Label: tensor(265) . Confidence Score: 2.458286762237549 %\n",
            "Label: tensor(152) . Confidence Score: 1.5673789978027344 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5_1vJqZI4R6"
      },
      "source": [
        "\r\n"
      ]
    }
  ]
}